<div align=center>
<img src="http://img.070077.xyz/202204021121013.png" width="50%" />
</div>

---

<div align=center><font size='70'>《数据科学导论》大作业报告</font></div>


---



### 成员列表

|    姓名    |    学号    |    班级    |
| :--------: | :--------: | :--------: |
| **王祥龙** | 2020211415 | 2020211322 |
|    刘亮    | 2020211318 | 2020211322 |

### 问题分析

根据贷款app提供的数据，预测用户是否存在贷款违约的可能。所给数据集的指标包括：收入、年龄、专业、房车、位置等。

各特征数据值的分布图：

![image-20220627221948417](http://img.070077.xyz/202206272219494.png)

<img src="http://img.070077.xyz/202206271241367.png"/>

### 数据预处理

通过Null的判断知道，Income居然有空的情况，初步分析是失业或退休人群导致的收入缺失。这个时候有两种选择：

- 用统计特征值代替
- 直接丢弃对应的列：Income的异常值占了约1/20的总量，还是有必要进行处理的，不能浪费

同样地，图示明显，还有异常值Age为0。Age异常值量20000+，不能浪费，观察异常值的具体分布，结果与上图相似度较高：

![image-20220629185010061](http://img.070077.xyz/image-20220629185010061.png)

因此我们采用了回归分析的方式填充异常值。

> 为什么不选取平均值的方式？

因为我们认为接下来的操作设计到分类聚合，将缺失值统一到同一值可能会导致分箱偏差。



进行填充后，我们还尝试了通过$3\sigma$原则，将离群点转化为均值，效果较小。

```python
u = df["Income"].mean()
std = df["Income"].std()
df[np.abs(df["Income"] - u) > 3*std]["Income"] = u
```



并且，对于存在字符串的列，我们使用了【独热编码】的方式进行离散化，方便我们进行后续的处理。



### 数据探索

首先分析各数据之间的相关性，当然要祭出经典的seaborn热力图：

![](http://img.070077.xyz/jn0HVs.png)

图中可以看到变量之间基本没什么直接的关系，不过有两个地方值得注意：当前工作年限与行业的经验呈现出较强的正相关关系，即工作时间越长经验越足，这点从常识上也能推出。另一个是右下角的高度负相关，这里是采用了独热编码的缘故，所以不展开分析。

接下来查看信贷风险与其余特征的分布关系。

![jn0ban.png](http://img.070077.xyz/jn0ban.png)

可以看到其实分布基本相同，只不过有信贷风险的年轻人多些，工作年限少的多一些。



同时Income作为非常关键的指标，我们不妨放大看看，参差嶙峋。

![image-20220629185507693](http://img.070077.xyz/image-20220629185507693.png)

### 模型

我们小组尝试过多种模型进行拟合。比如：
- 考虑到二分性，使用了`logistic`回归，结果AUC仅在0.63附近，效果不理想。可能是因为独热编码后，维度过高；

- 考虑到需要概率，我们尝试了多元线性回归模型，AUC依旧没有改善。我们意识到，难以通过函数的方式获得良好的效果了。
- 我们也考虑了SVM方式，但是运行时间过久，难以继续探索。
- 于是我们转而走向集成学习的道路。首先我们使用了常规的随机森林算法，但AUC仍为0.63左右，不尽如人意；了解到了`XgBoost`后,这一模型运作良好，让我们的AUC值突破了0.9
- 我们不满足于此，了解到`XgBoost`的优化版本`LightGBM`，其具有时间成本低，准确率堪比`Xgboost`的性能。



得到了较好的模型后，我们开始不断地优化它、调整参数、调整数据，其中的探索有：

- 我们把“婚否”、“是否有车”等也进行了独热编码，但是它实际上是二元属性，只需要直接映射为0和1就可以的。

```python
df["Married/Single"]=df["Married/Single"].map({"single":1,"married":0})
df["Car_Ownership"]=df["Car_Ownership"].map({"yes":0,"no":1})  
```

- 我们认为由独热编码产生的高维度影响了模型的表现，删除了这部分列。但是效果是负效果，事实也应是如此，没有这种捷径。



最后我们的模型是这样的（注释是一些小细节）：

```python
from sklearn.linear_model import *
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split 
import re
import lightgbm as lgb
df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
# 我们采用的独热编码是get_dummy，其列名不适用于LightGBM，因此有上行
X = df.drop(columns=['Risk_Flag'])
Y = df['Risk_Flag']

Y[Y < 0] = 0
Y[Y > 0] = 1
Y = Y.astype(int)
X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)
d_train = lgb.Dataset(X_train, label=y_train)
params = { 'learning_rate' : 0.1,
         "objective": "binary",
          'reg_alpha': 0.11981995466204232, 'reg_lambda': 0.01803637626441701, 'num_leaves': 297, 'min_child_samples': 7, 'max_depth': 37, 'colsample_bytree': 0.3683474729890541, 'n_estimators': 3617, 'cat_smooth': 51, 'cat_l2': 14, 'min_data_per_group': 72}
clf = lgb.train(params, d_train, 4000)
```

其中，我们使用了`optuna`进行自动调参。在两人一起将程序运行了整整一晚之后，我们得到了想要的参数，并欣喜地获得了0.916的结果。

### 成员分工与心得

刘亮：字符串数值处理以及部分可视化、`lgbm`分类算法的选择、模型训练与测试、报告主体撰写

王祥龙：关系图等部分可视化、线性回归、逻辑回归、随机森林、`Xgboost`分类算法的选择与测试效果、模型训练与测试、报告补充说明



心得：

实验过程可以说充满了困难与挑战。在预处理后，我们先从回归算法开始，发现其理想程度很差。随后我们使用了随机森林算法，发现分类效果还是不尽如人意。于是我们将精力重点放在了分类器的选择上，通过上网查询到了`Xgboost`和`lgbm`算法，最后选择了性价比最高的`lgbm`算法。这之后我们将重点放在了预处理上。可惜的是，因为一心想降维精简算法复杂度与提高AUC，我们走进了删除城市、州等种类较多的字符串特征的错误认识，因为在删除后训练集的测试集的AUC明显提高了，一度接近94%。在我们以为将要成功之时，提交到服务器的AUC竟然只有80%多。我们才意识到走上了一条错误的道路，立刻恢复字符串特征的使用，最后的结果也没有到达让我们满意的程度。

数据科学家定义为具有计算机科学技术，数学和统计学知识基础和实质性专业理论知识的人。对于每一个可视化的图表、分析的指标，挖掘蕴含的信息需要独到的眼光，才能看到别人忽略的细节。在排行榜上，我们没有得到很高的分数，究其原因还是我们对于分类算法背后的原理没有理解透彻，以及预处理还有所缺陷导致的。我们相信随着经验的增长，提升模型的真实性水平会越来越常见。我们也会不断地向这个方向努力。

实验中结合实际，提供贷款的预测，我们也尝试代入自己的情况数据进行模型预测，对未来也有了清晰的认识。相信大数据一定会在越来越多的领域得到广泛的使用。

---
参考资料：
http://www.shichuan.org/Introduction-to-Data-Science.html